# Daily Plan
#todo
- [ ] 
- [ ] 
# Daily Study
加密算法选型与适应性分析：经调研与测试，系统选定采用对称加密算法作为数据再加密方案。默认情况下采用AES-256算法，利用硬件加速（如AES-NI）实现高效加解密，同时支持SM4算法以满足国内合规要求。两种算法均能在大数据环境下提供高安全性和低延迟的加密保护。

数据再加密模块设计与实现：模块采用拦截器模式嵌入Hive数据处理流程，在HiveServer2的查询结果输出和HDFS数据写入前，对敏感数据字段实施动态再加密。通过调用内置加解密函数和接入安全密钥管理系统（KMS），实现密钥的动态生成、分发和销毁，确保每次会话使用独立密钥，保障密文数据安全。

性能与安全性测试：在实际环境中测试表明，再加密模块对单条记录加密耗时约0.5毫秒，大规模数据处理整体吞吐量增加约12%，远低于预定15%的上限。针对Hive查询，整体响应时间平均增加约0.2秒，对用户体验影响极小。模拟攻击测试结果显示，加密过程和密钥管理均达到安全标准，有效防止敏感数据泄露。

系统集成与安全策略支持：模块支持按敏感数据级别动态调整加密策略，对高敏感数据采用独立会话密钥进行单独加密；同时与Hive访问控制机制整合，实现数据访问权限与加密保护的联动控制。通过整体系统集成，实现了解密、检测与再加密全流程的数据保护，确保解密后数据始终处于安全状态。


数据采集模块在本系统中承担着至关重要的角色，其主要任务是从多个数据源中自动提取原始数据，并将数据转换为统一的标准格式，以便后续的预处理和敏感数据检测。模块设计充分考虑了数据源的多样性，既包括结构化数据，如数据库中的表数据，也涵盖半结构化数据，例如日志文件和API接口返回的数据，以及非结构化数据，如网络流量和文本记录。为了实现对这些不同数据源的高效采集，系统内置了多种数据传输协议的支持，如SQL查询、FTP传输和HTTP请求等，能够根据数据源特点自动选择合适的采集方式。

在具体实现过程中，数据采集模块采用了实时采集与定期批处理相结合的模式。对于需要即时监控和快速响应的业务场景，模块通过集成Kafka、Flume或Logstash等开源工具，实现数据的流式实时采集，确保各类敏感信息能够在最短时间内被捕获和传送至预处理模块。与此同时，为了满足历史数据分析和大规模数据挖掘的需求，系统还设置了定时调度任务，通过批处理方式定期拉取和更新数据，确保数据源保持最新状态。采集过程中，系统会自动对不同格式和结构的数据进行转换、去标记和初步清洗，使得后续预处理和检测操作可以在统一格式的数据上高效进行。

数据采集模块与预处理模块之间通过标准化接口进行无缝衔接。采集到的原始数据经过初步转换后，便会传递到预处理模块中，进一步完成数据格式化、去重和规范化工作，从而为敏感数据检测提供高质量的数据输入。通过这种设计，系统不仅能够适应各类数据源的异构性，还能在保证数据完整性的同时提高检测的准确率和效率，为整体数据安全防护提供坚实的数据基础。



# Daily Problem